{
 "metadata": {
  "name": "",
  "signature": "sha256:c338f42a5b532950a32e245370c534a7e10bffa2b260ede15f8a25495993e960"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "\n",
      "import numpy as np\n",
      "from scipy.io                     import wavfile\n",
      "from numpy.lib                    import stride_tricks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Abstract"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Significant digitization efforts have resulted in large multimodal\n",
      "music collections comprising visual (scanned sheet\n",
      "music) as well as acoustic material (audio recordings).\n",
      "In this paper, we present a novel procedure for mapping\n",
      "scanned pages of sheet music to a given collection of audio\n",
      "recordings by identifying musically corresponding audio\n",
      "clips. To this end, both the scanned images as well as the\n",
      "audio recordings are first transformed into a common feature\n",
      "representation using optical music recognition (OMR) and\n",
      "methods from digital signal processing, respectively. Based\n",
      "on this common representation, a direct comparison of the\n",
      "two different types of data is facilitated. This allows for a\n",
      "search of scan-based queries in the audio collection. We report\n",
      "on systematic experiments conducted on the corpus of\n",
      "Beethoven\u2019s piano sonatas showing that our mapping procedure\n",
      "works with high precision across the two types of\n",
      "music data in the case that there are no severe OMR errors.\n",
      "The proposed mapping procedure is relevant in a real-world\n",
      "application scenario at the Bavarian State Library for automatically\n",
      "identifying and annotating scanned sheet music by\n",
      "means of already available annotated audio material."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last years have seen increasing efforts in building up\n",
      "large digital music collections. These collections typically\n",
      "contain various types of data ranging from audio data such\n",
      "as CD recordings to image data such as scanned sheet music,\n",
      "thus concerning both the auditorial and the visual modal-\n",
      "ities. In view of multimodal searching, navigation, and\n",
      "browsing applications across the various types of data, one\n",
      "requires powerful tools that support the process of analyz-\n",
      "ing, correlating, and annotating the available material. In the\n",
      "case of digitized audio recordings, first services have been\n",
      "established to automate the annotation process by identify-\n",
      "ing each recording and assigning available metadata such as\n",
      "title, artist, or lyrics. Here, the metadata is drawn from spe-\n",
      "cialized annotation databases provided by commercial ser-\n",
      "vices such as Gracenote [6] or DE-PARCON [9].\n",
      "\n",
      "Opposed to acoustic music data, which is increasingly\n",
      "available in digital formats, most sheet music is still pro-\n",
      "duced and sold in printed form. In the last years, dig-\n",
      "ital music libraries have started to systematically digitize\n",
      "their holdings of sheet music resulting in a large number\n",
      "of scanned raster images. To make the raw image data\n",
      "available to content-based retrieval and browsing, meth-\n",
      "ods for automatically extracting and annotating semanti-\n",
      "cally meaningful entities contained in the scanned docu-\n",
      "ments are needed. In this context,\n",
      "optical music recogni-\n",
      "tion\n",
      "(OMR) [3] is a key task. Here, the goal is to convert\n",
      "scanned sheet music into a computer readable symbolic mu-\n",
      "sic format such as MIDI or MusicXML [13]. Even though\n",
      "significant progress has been made in the last years, cur-\n",
      "rent OMR algorithms are substantially error-prone, resulting\n",
      "in systematic errors that require subsequent correction [2].\n",
      "Similarly, there is still a high demand for reliable solutions\n",
      "for the more general task of automatic sheet music annota-\n",
      "tion in the digital library community.\n",
      "\n",
      "In this paper, we present a novel approach for automat-\n",
      "ically annotating scanned pages of sheet music with meta-\n",
      "data. Our approach is based on a new procedure for\n",
      "mapping\n",
      "the scanned sheet music pages to an existing collection of\n",
      "annotated audio recordings. The mapping allows for iden-\n",
      "tifying and subsequently annotating the scans based on the\n",
      "metadata and annotations that are already available for the\n",
      "audio recordings. In particular, as it is the case in the spe-\n",
      "cific application scenario at the Bavarian State Library, we\n",
      "assume the existence of an audio collection containing an-\n",
      "notated digitized audio recordings for all pieces to be con-\n",
      "sidered in the sheet music digitization process. The con-\n",
      "version of both the audio recordings (by employing filtering\n",
      "methods) and the scanned images (by employing OMR) to\n",
      "a common feature representation allows for a direct com-\n",
      "parison of the two different types of data. Using the feature\n",
      "sequence obtained from a few consecutive staves or an en-\n",
      "tire page of the scanned sheet music as query, we compute\n",
      "the top match within the documents of the audio database.\n",
      "The top match typically lies within a musically correspond-\n",
      "ing audio recording, which then allows for identifying the\n",
      "scanned page and for transferring all available audio anno-\n",
      "tations to the scanned sheet music domain. This procedure\n",
      "is described in Sect. 2 and illustrated by Fig. 1. We have\n",
      "tested and analyzed our mapping procedure by means of a\n",
      "real-world application scenario using the corpus of the\n",
      "32\n",
      "piano sonatas by Ludwig van Beethoven. In Sect. 3, we\n",
      "discuss the outcome of our experiments showing that the\n",
      "mapping across the two music domains is robust even in the\n",
      "presence of local OMR errors, but suffers in the presence of\n",
      "severe global OMR errors. We also describe a postprocess-\n",
      "ing procedure that allows for detecting most of the misclas-\n",
      "sifications and automatically reveals most of the passages\n",
      "within the scanned pages where the severe OMR errors oc-\n",
      "curred. In Sect. 4, we conclude this paper with prospects on\n",
      "future work and indicate how to improve the identification\n",
      "rate by correcting and compensating for severe OMR errors\n",
      "prior to the mapping stage."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Mapping Procedure"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One key strategy of our mapping procedure is to reduce the\n",
      "two different types of music data, the audio recordings as\n",
      "well as the scanned sheet music, to the same type of feature\n",
      "representation, which then allows for a\n",
      "direct\n",
      "comparison\n",
      "across\n",
      "the two domains. In this context, chroma-based fea-\n",
      "tures have turned out to be a powerful mid-level music rep-\n",
      "resentation [1, 7, 12]. Here, the\n",
      "chroma\n",
      "correspond to the\n",
      "twelve traditional pitch classes of the equal-tempered scale\n",
      "and are commonly indicated by the twelve pitch spelling at-\n",
      "tributes C,\n",
      "C\n",
      ",D,\n",
      "...\n",
      ",B as used in Western music notation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Chroma Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use Chroma features by Alexander Lerch, from http://www.audiocontentanalysis.org/code/audio-features/pitch-chroma/ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Requires Short-Time Fourier Transform"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" short time fourier transform of audio signal \"\"\"\n",
      "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\n",
      "    win = window(frameSize)\n",
      "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
      "    \n",
      "    # zeros at beginning (thus center of 1st window should be for sample nr. 0)\n",
      "    samples = np.append(np.zeros(np.floor(frameSize/2.0)), sig)    \n",
      "    # cols for windowing\n",
      "    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\n",
      "    # zeros at end (thus samples can be fully covered by frames)\n",
      "    samples = np.append(samples, np.zeros(frameSize))\n",
      "    \n",
      "    frames = stride_tricks.as_strided(samples, shape=(cols, frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n",
      "    frames *= win\n",
      "    \n",
      "    return np.fft.rfft(frames) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Chroma Features extraction Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def GeneratePcFilters(iFftLength, f_s):\n",
      "\n",
      "    # initialization at C4\n",
      "    f_mid           = 261.63\n",
      "    iNumOctaves     = 4\n",
      "\n",
      "    #sanity check\n",
      "    while (f_mid * 2**iNumOctaves > f_s/2 ):\n",
      "        iNumOctaves = iNumOctaves - 1\n",
      "\n",
      "    H               = np.zeros((12, iFftLength)) \n",
      "\n",
      "    for i in range(12):\n",
      "\n",
      "        afBounds  = np.array([2.0**(-1/24.0), 2.0**(1/24.0)]) * f_mid * 2.0 * iFftLength/float(f_s)\n",
      "\n",
      "        for j in range(iNumOctaves):\n",
      "           iBounds                      = np.array([ceil(2**(j)*afBounds[0]), floor(2**(j)*afBounds[1])]).astype(int)\n",
      "           H[i,iBounds[0]-1:iBounds[1]]   = 1/(iBounds[1]+1.0-iBounds[0])\n",
      "\n",
      "        # increment to next semi-tone\n",
      "        f_mid   = f_mid*2**(1/12.0)\n",
      "\n",
      "    return H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chroma(magnitude_spectrum,f_s):\n",
      "    \n",
      "   # allocate memory\n",
      "    chroma  = np.zeros((12, magnitude_spectrum.shape[0]))\n",
      "\n",
      "    # generate filter matrix\n",
      "    H       = GeneratePcFilters(magnitude_spectrum.shape[1], f_s)\n",
      "    \n",
      "    # compute pitch chroma\n",
      "    chroma  = np.dot(H ,(np.abs(magnitude_spectrum.transpose())**2))\n",
      "\n",
      "    # norm pitch chroma to a sum of 1\n",
      "    chroma  = chroma / np.tile(np.sum(chroma, axis=0), (12,1))\n",
      "\n",
      "    # avoid NaN for silence frames\n",
      "    chroma[:,np.sum(magnitude_spectrum,axis=0) == 0] = 0\n",
      "\n",
      "    return chroma"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of audio recordings, normalized chroma-based\n",
      "features indicate the short-time energy distribution among\n",
      "the twelve chroma and closely correlate to the harmonic pro-\n",
      "gression of the underlying piece. Based on signal processing\n",
      "techniques, the transformation of an audio recording into a\n",
      "chroma representation (or chromagram) may be performed\n",
      "either by using short-time Fourier transforms in combina-\n",
      "tion with binning strategies [1] or by employing suitable\n",
      "multirate filter banks [12]. In our implementation, we use\n",
      "a quantized and smoothed version of chroma features, re-\n",
      "ferred to as CENS features, see [12]. The transformation\n",
      "of scanned sheet music into a corresponding chromagram\n",
      "requires several steps, see [11]. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, each scanned page\n",
      "is analyzed using optical music recognition (OMR) [2, 3].\n",
      "In our system, we use the commercially available Sharp-\n",
      "Eye software [8] to extract musical note parameters (onset\n",
      "times, pitches, durations) along with 2D position parame-\n",
      "ters as well as bar line information from the scanned im-\n",
      "age. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***TODO***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming a fixed tempo of\n",
      "100\n",
      "BPM, the explicit\n",
      "pitch and timing information can be used to derive a chro-\n",
      "magram essentially by identifying pitches that belong to the\n",
      "same chroma class. A similar approach has been proposed\n",
      "in [7] for transforming MIDI data into a chroma represen-\n",
      "tation. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***TODO***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fig. 1 shows a resulting scan chromagram obtained\n",
      "for the first page of the second movement of Beethoven\u2019s pi-\n",
      "ano sonata Op. 2 No. 1. Note that the tempo assumption (of\n",
      "always choosing\n",
      "100\n",
      "BPM) is not a severe restriction since\n",
      "the mapping algorithm to be described next can handle local\n",
      "and global tempo variations anyway.\n",
      "\n",
      "\n",
      "For the actual scan-audio mapping, we use a match-\n",
      "ing procedure similar to the one described in [10]. First,\n",
      "in a preprocessing step, all recordings of the given audio\n",
      "database are converted into sequences of CENS vectors. (In\n",
      "our implementation, we use a feature sampling rate of 1\n",
      "Hz.) While keeping book on document boundaries, all these\n",
      "CENS sequences are concatenated into a single audio fea-\n",
      "ture sequence. Then, each scanned page of sheet music to\n",
      "be identified is also converted into a sequence of CENS fea-\n",
      "tures. The resulting scan feature sequence is then compared\n",
      "to the audio feature sequence using subsequence dynamic\n",
      "time warping (DTW). For a detailed account on this variant\n",
      "of DTW we refer to [12]. In our experiments, it turned out\n",
      "that the DTW step sizes\n",
      "(2\n",
      ",\n",
      "1)\n",
      ",\n",
      "(1\n",
      ",\n",
      "2)\n",
      ",\n",
      "(1\n",
      ",\n",
      "1)\n",
      "(instead of the\n",
      "classical step sizes\n",
      "(1\n",
      ",\n",
      "0)\n",
      ",\n",
      "(0\n",
      ",\n",
      "1)\n",
      ",\n",
      "(1\n",
      ",\n",
      "1)\n",
      ") lead to more robust\n",
      "matching results, and are hence used in the remainder of\n",
      "this paper. \n",
      "As a result of the DTW computation, one obtains\n",
      "a\n",
      "matching curve\n",
      ".The\n",
      "i\n",
      "th position of the matching curve\n",
      "contains the costs for matching the scan feature sequence to\n",
      "the most similar subsequence of the audio feature sequence\n",
      "ending at position\n",
      "i\n",
      ". Therefore, the curve\u2019s local minima\n",
      "close to zero correspond to audio feature subsequences sim-\n",
      "ilar to the scan feature sequence. These subsequences are\n",
      "referred to as\n",
      "matches\n",
      ". Because of the book-keeping, doc-\n",
      "ument numbers and positions of matches within each au-\n",
      "dio document can be recovered easily. Note that DTW can\n",
      "compensate for possible temporal differences between scan\n",
      "feature sequences and corresponding audio feature subse-\n",
      "quences thus also relativizing the above tempo assumption"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. Experiments and Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic identification and annotation procedure of a given\n",
      "scanned page of sheet music can be summarized as follows.\n",
      "First, map a given scanned page against the audio database\n",
      "and derive the top match of lowest cost. Then determine the\n",
      "audio recording that contains the top match and transfer all\n",
      "annotations available for the audio recording to the image\n",
      "domain. Note that in the ideal case the top match not only\n",
      "identifies the scanned page but also indicates the time posi-\n",
      "tion within the audio recording where the music notated on\n",
      "the page is actually played.\n",
      "\n",
      "\n",
      "We now show to which extent this approach also works\n",
      "in practice by discussing a real-world application scenario\n",
      "using the musically relevant corpus of Beethoven\u2019s piano\n",
      "sonatas. Our test database and some technical details are\n",
      "described in Sect. 3.1. In Sect. 3.2, we discuss a baseline\n",
      "experiment using MIDI versions instead of extracted OMR\n",
      "data. This experiment indicates which identification rates\n",
      "one may expect in the optimal (but unrealistic) case where\n",
      "no OMR extraction errors have occurred. Then, in Sect. 3.3,\n",
      "we describe several experiments performed on the actual\n",
      "OMR data. Here, we also discuss various types of OMR er-\n",
      "rors that significantly degrade the mapping quality. Finally,\n",
      "in Sect. 3.4, we describe a postprocessing strategy that au-\n",
      "tomatically reveals most of the misclassifications."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.1 Test Database"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our experiments have been conducted on the basis of the\n",
      "32\n",
      "piano sonatas by Ludwig van Beethoven, which play a key\n",
      "role in the evolution of the sonata form and are considered as\n",
      "one of the greatest treasures in the music literature. Because\n",
      "of its outstanding musical significance and the large number\n",
      "of available digitized audio recordings, the automated anal-\n",
      "ysis and organization of the corpus of Beethoven\u2019s piano\n",
      "sonatas is highly relevant to musicologists and librarians.\n",
      "\n",
      "\n",
      "Our audio database consists of a complete recording of\n",
      "the\n",
      "32\n",
      "piano sonatas conducted by Daniel Barenboim, com-\n",
      "prising\n",
      "101\n",
      "audio documents (basically corresponding to the\n",
      "movements) and\n",
      "11\n",
      "hours of audio material. Furthermore,\n",
      "we have a scanned version of the corresponding sheet music\n",
      "(Volume 1 & 2, G. Henle Verlag) at our disposal amount-\n",
      "ing to a total number of\n",
      "604\n",
      "digitized pages (\n",
      "3693\n",
      "two-stave\n",
      "systems). In the following, dealing with piano music, the\n",
      "term\n",
      "line\n",
      "is used to denote a two-stave system consisting\n",
      "of a stave for the right and a stave for the left hand. The\n",
      "scanned pages, which are available as 600dpi b/w images in\n",
      "the TIFF format, have been processed by the OMR Engine\n",
      "of SharpEye 2.68 and saved in the MusicXML file format\n",
      "as one file per page. Finally, each of the\n",
      "604\n",
      "MusicXML\n",
      "files was transformed into a sequence of CENS vectors (one\n",
      "feature per second) assuming a fixed tempo of\n",
      "100\n",
      "BPM.\n",
      "Subsequently, using the extracted OMR information on the\n",
      "notated systems, the CENS sequences were segmented into\n",
      "3693\n",
      "subsequences corresponding to the lines."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.2 Baseline Experiment: MIDI-Audio Mapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a baseline experiment, we investigated what identifica-\n",
      "tion rates one may expect in the case that there are no se-\n",
      "vere OMR extraction errors. To this end, we used a com-\n",
      "plete set of MIDI files for the\n",
      "32\n",
      "Beethoven sonatas and ran-\n",
      "domly generated a large number of MIDI fragments of vari-\n",
      "ous lengths, which were used instead of the OMR extraction\n",
      "results. Then, for each of these MIDI fragments we com-\n",
      "puted a matching curve with respect to the audio database\n",
      "and determined the topmost audio match. Recall that in the\n",
      "identification scenario the objective is to determine the piece\n",
      "of music underlying the respective MIDI fragment by using\n",
      "the audio recordings of the database as an identifier. There-\n",
      "fore, we consider a match as\n",
      "correct\n",
      "if it lies within the au-\n",
      "dio document that corresponds to the same movement as the\n",
      "MIDI document from which the respective query is taken.\n",
      "Otherwise the match is considered as\n",
      "incorrect\n",
      ".\n",
      "\n",
      "\n",
      "In particular, we investigated the dependence of the num-\n",
      "ber of correct audio matches subject to the length\n",
      "L\n",
      "(given\n",
      "in seconds) of the MIDI query. To this end, we randomly\n",
      "generated\n",
      "1000\n",
      "MIDI queries for each of the seven param-\n",
      "eters\n",
      "L\n",
      "\u2208{\n",
      "10\n",
      ",\n",
      "20\n",
      ",\n",
      "30\n",
      ",...,\n",
      "70\n",
      "}\n",
      ". Each of the queries lies\n",
      "within a single MIDI file and therefore has a unique cor-\n",
      "rect assignment to one of the\n",
      "101\n",
      "movements. The second\n",
      "column of Table 1 shows the number of correct matches. As\n",
      "an example, consider the case\n",
      "L\n",
      "=10\n",
      ", where\n",
      "823\n",
      "of the\n",
      "1000\n",
      "matches were correct. Note that the number of correct\n",
      "matches increases significantly with the query length. For\n",
      "example, for\n",
      "L\n",
      "=40\n",
      "only\n",
      "3\n",
      "of the\n",
      "1000\n",
      "queries were mis-\n",
      "classified. To give a more detailed picture of the matching\n",
      "quality, Table 1 additionally provides various cost and con-\n",
      "fidence values. The third, fourth, and fifth column show the\n",
      "average cost values, the standard deviations, and the maxi-\n",
      "mal cost values for the correct top matches. For example,\n",
      "in the case\n",
      "L\n",
      "=10\n",
      ", the average cost value (standard de-\n",
      "viation/maximal cost value) for the\n",
      "823\n",
      "correct matches is\n",
      "0\n",
      ".\n",
      "059\n",
      "(\n",
      "0\n",
      ".\n",
      "024\n",
      "/\n",
      "0\n",
      ".\n",
      "223\n",
      "). The latter cost values are with respect\n",
      "to a range from 0 (no costs) to 1 (maximum costs). Increas-\n",
      "ing\n",
      "L\n",
      "leads to slightly higher cost values stabilizing around\n",
      "the value\n",
      "0\n",
      ".\n",
      "07\n",
      "even for long queries.\n",
      "\n",
      "\n",
      "Similarly, the sixth, seventh, and eighth columns of Ta-\n",
      "ble 1 show the corresponding values for the incorrect top\n",
      "matches. For example, in the case\n",
      "L\n",
      "=10\n",
      ", the average cost\n",
      "of the\n",
      "177\n",
      "incorrect top matches is\n",
      "0\n",
      ".\n",
      "084\n",
      "with a standard de-\n",
      "viation of\n",
      "0\n",
      ".\n",
      "034\n",
      ". Note that in the case of incorrect matches,\n",
      "when increasing the query length, the average cost increases\n",
      "at a much higher rate than in the case of correct matches.\n",
      "\n",
      "\n",
      "We also investigated how well the correct matches were\n",
      "separated by successive matches that do not lie in the respec-\n",
      "tive correct audio document. To this end, we computed for\n",
      "each query the minimal cost value of a restricted matching\n",
      "curve, where the correct audio document had been removed.\n",
      "Then, for all correctly identified queries, we computed the\n",
      "difference of this minimal value and the cost of the correct\n",
      "top match. This difference value, which we refer to as\n",
      "con-\n",
      "fidence gap\n",
      ", indicates the identification reliability based on\n",
      "the top match. The average confidence value is shown in the\n",
      "last column of Table 1. For example, in the case\n",
      "L\n",
      "=10\n",
      "the average confidence gap amounts to the value\n",
      "0\n",
      ".\n",
      "044\n",
      ".In-\n",
      "creasing\n",
      "L\n",
      "leads to a significant increase of the confidence\n",
      "gapuptothevalueof\n",
      "0\n",
      ".\n",
      "135\n",
      "for\n",
      "L\n",
      "=70\n",
      ". In conclusion, one\n",
      "may say that one obtains very good identification rates (with\n",
      "an error rate of less than\n",
      "1%\n",
      ") for MIDI fragments of at least\n",
      "30\n",
      "seconds of duration."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.3 OMR-Audio Mapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we describe a similar experiment, now using the\n",
      "(potentially flawed) OMR extraction results instead of the\n",
      "\u201cclean\u201d MIDI data. For each of the\n",
      "604\n",
      "scanned pages, we\n",
      "computed a CENS feature sequence as explained in Sect. 2.\n",
      "Then, from these sequences, we randomly generated\n",
      "1000\n",
      "subsequences of length\n",
      "L\n",
      "for each of the length parameters\n",
      "L\n",
      "\u2208{\n",
      "10\n",
      ",\n",
      "20\n",
      ",...,\n",
      "70\n",
      "}\n",
      ". Table 2 summarizes the OMR-audio\n",
      "mapping results. Obviously, the identification rate drops\n",
      "significantly compared to the pure MIDI case. For exam-\n",
      "ple, in the case\n",
      "L\n",
      "=10\n",
      "only\n",
      "484\n",
      "out of the\n",
      "1000\n",
      "OMR\n",
      "query fragments appear as top match in the correct audio\n",
      "document (opposed to the\n",
      "823\n",
      "correct matches in the MIDI\n",
      "case). The identification rate increases to roughly\n",
      "87%\n",
      "for\n",
      "OMR feature sequences that correspond to a duration of\n",
      "50\n",
      "seconds and above. A comparison with Table 1 shows that,\n",
      "in the OMR case, the average costs of the correct matches\n",
      "are much higher than the ones in the MIDI case. Further-\n",
      "more, the confidence gap is much smaller.\n",
      "\n",
      "\n",
      "All these numbers indicate that the OMR-audio map-\n",
      "ping procedure significantly suffers from the artifacts that\n",
      "are mainly caused by OMR extraction errors. In particu-\n",
      "lar, a manual investigation of samples of the OMR extrac-\n",
      "tion results revealed that there are two prominent types of\n",
      "OMR errors that significantly degrade the quality of the\n",
      "CENS feature sequences. First, for roughly\n",
      "7%\n",
      "of the lines\n",
      "(two-stave systems) the key signature was extracted incor-\n",
      "rectly. In particular, one or even more accidentals notated\n",
      "at the beginning of each stave were missing. Such an er-\n",
      "ror generally distorts the CENS subsequence for an entire\n",
      "line, since a missing accidental causes all notes of a spe-\n",
      "cific pitch class to be shifted upwards or downwards by\n",
      "one semitone, which may significantly corrupt the chroma\n",
      "distribution. Second, in almost\n",
      "5%\n",
      "of the measures there\n",
      "were some note or beam extraction errors that resulted in\n",
      "inconsistencies with respect to the notated time signature.\n",
      "In such cases, the conversion tool of our OMR software,\n",
      "which transforms the OMR extraction parameters into a Mu-\n",
      "sicXML file, simply discards all voices within those mea-\n",
      "sures that reveal such inconsistencies. This also results in\n",
      "a significant corruption of the chroma distribution. Obvi-\n",
      "ously, the automated detection and correction of such OMR\n",
      "extraction errors would overcome these problems resulting\n",
      "in significantly improved identification rates. These issues\n",
      "are left for future work and are further discussed in Sect. 4.\n",
      "\n",
      "\n",
      "We continue the analysis of our OMR-audio mapping\n",
      "procedure based on the raw OMR material. Instead of using\n",
      "randomly chosen OMR fragments of a specific duration, we\n",
      "now investigate the mapping quality based on musical units\n",
      "such as pages or lines. Using entire pages in the OMR-audio\n",
      "mapping leads to an identification rate of roughly\n",
      "82\n",
      ".\n",
      "5%\n",
      ".\n",
      "The average length of the corresponding CENS sequences\n",
      "amounts to\n",
      "55\n",
      "seconds yielding robust mappings if there are\n",
      "no severe OMR errors. Another problem that often leads\n",
      "to misclassifications is that a single scanned page may re-\n",
      "fer to more than one pieces of music. In particular for our\n",
      "Beethoven corpus, a single page may contain both the end\n",
      "and the beginning of two consecutive movements. To over-\n",
      "come this problem, one may use single lines in the mapping\n",
      "process instead of entire pages. This also yields the advan-\n",
      "tage of having several identifiers per page. On the downside,\n",
      "the average length of the CENS sequences corresponding to\n",
      "the lines lies below a duration of\n",
      "10\n",
      "seconds yielding an\n",
      "identification rate of only\n",
      "44\n",
      ".\n",
      "57%\n",
      ", see Table 3. To improve\n",
      "the identification rate of the line-based mapping strategy, we\n",
      "query each line in the context of\n",
      "preceding and\n",
      "subse-\n",
      "quent lines. In other words, instead of using a single line\n",
      "we use a block of\n",
      "2\n",
      "+1\n",
      "subsequent lines with the refer-\n",
      "ence line positioned in the middle. Here, we assume that\n",
      "all pages belonging to one movement are in the correct or-\n",
      "der, hence allowing us to consider blocks of lines ranging\n",
      "across two consecutive pages. To systematically investigate\n",
      "the identification rate depending on the number of lines used\n",
      "in the OMR-audio mapping, for each of the\n",
      "3693\n",
      "lines of our\n",
      "scanned Beethoven material, we generated CENS query se-\n",
      "quences corresponding to\n",
      "1\n",
      ",\n",
      "3\n",
      ",\n",
      "5\n",
      ", and\n",
      "7\n",
      "lines. Table 3 shows\n",
      "both the resulting identification rates based on the top match\n",
      "(\n",
      "k\n",
      "=1\n",
      ") and the recall values for the correct audio document\n",
      "for the top\n",
      "k\n",
      "matches with\n",
      "k\n",
      "\u2208{\n",
      "1\n",
      ",\n",
      "2\n",
      ",\n",
      "5\n",
      ",\n",
      "10\n",
      ",\n",
      "20\n",
      ",\n",
      "50\n",
      "}\n",
      ".Forex-\n",
      "ample, using three lines, the top match (\n",
      "k\n",
      "=1\n",
      ") was correct\n",
      "in\n",
      "71\n",
      ".\n",
      "30%\n",
      "of the\n",
      "3693\n",
      "OMR-audio mappings. Considering\n",
      "the top\n",
      "5\n",
      "matches (\n",
      "k\n",
      "=5\n",
      "), at least one of these matches was\n",
      "correct in\n",
      "83\n",
      ".\n",
      "62%\n",
      "of the mappings."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.4 Postprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now show how the additional information of consider-\n",
      "ing the\n",
      "k\n",
      "top matches (instead of considering only the top\n",
      "match) can be used to detect most of the incorrect identi-\n",
      "fications. The only assumption we use is that the scanned\n",
      "pages that correspond to a specific movement are given as\n",
      "a sequence of consecutive pages, i.e., pages of different\n",
      "movements are not interleaved. We explain our postprocess-\n",
      "ing procedure by means of our Beethoven scenario using\n",
      "Q\n",
      "= 3693\n",
      "OMR queries each consisting of\n",
      "7\n",
      "subsequent\n",
      "lines and considering the\n",
      "k\n",
      "=5\n",
      "top matches. Recall that the\n",
      "objective is to map each of the queries to one of the\n",
      "P\n",
      "= 101\n",
      "audio documents (representing the pieces or movements).\n",
      "We construct a\n",
      "P\n",
      "\u00d7\n",
      "Q\n",
      "mapping matrix\n",
      "M\n",
      ",wheretherows\n",
      "correspond to the pieces and the columns to the queries.\n",
      "Then an entry\n",
      "M\n",
      "(\n",
      "p, q\n",
      ")\n",
      ",\n",
      "1\n",
      "\u2264\n",
      "p\n",
      "\u2264\n",
      "P\n",
      ",\n",
      "1\n",
      "\u2264\n",
      "q\n",
      "\u2264\n",
      "Q\n",
      ", is non-\n",
      "zero if and only if the\n",
      "p\n",
      "th\n",
      "audio document appears among\n",
      "the top\n",
      "k\n",
      "matches for the\n",
      "q\n",
      "th\n",
      "query. In this case\n",
      "M\n",
      "(\n",
      "p, q\n",
      ")\n",
      "is\n",
      "set to\n",
      "1\n",
      "\u2212\n",
      "c\n",
      ", where\n",
      "c\n",
      "\u2208\n",
      "[0\n",
      ",\n",
      "1]\n",
      "denotes the cost of the corre-\n",
      "sponding match. In case there are several matches for the\n",
      "entry\n",
      "(\n",
      "p, q\n",
      ")\n",
      "among the top\n",
      "k\n",
      "matches, we define\n",
      "c\n",
      "to be the\n",
      "minimal cost value over these matches. Note that\n",
      "M\n",
      "(\n",
      "p, q\n",
      ")\n",
      "expresses a kind of confidence that the\n",
      "q\n",
      "th\n",
      "query belongs the\n",
      "p\n",
      "th\n",
      "piece. Furthermore,\n",
      "M\n",
      "indicates the kind of confusion\n",
      "that occurred in the identification procedure. Fig. 2 shows\n",
      "the mapping matrix for the Beethoven scenario.\n",
      "\n",
      "\n",
      "For our Beethoven corpus, both the audio recordings and\n",
      "the scanned pages are sorted with respect to increasing opus\n",
      "and movement numbers. Therefore, a correct mapping of\n",
      "all queries corresponds to a diagonal staircase-like structure\n",
      "in\n",
      "M\n",
      ". \n",
      "\n",
      "In the following, we do not assume that the scanned\n",
      "pages are given in the same order (on the piece and move-\n",
      "ment level) as the audio recordings, since this assumption\n",
      "is often violated in real-world digitization applications. For\n",
      "example, many music books contain a more or less unsorted\n",
      "mixture of various pieces and movements. Therefore, we\n",
      "only make the assumption that the pages that correspond to a\n",
      "specific audio document (referring to a specific movement)\n",
      "are given in the correct order. Then, in case of a correct\n",
      "identification of the OMR queries, the matrix\n",
      "M\n",
      "reveals a\n",
      "structure of horizontal line segments, where each such seg-\n",
      "ment corresponds to an audio document.\n",
      "In the following, a tuple\n",
      "(\n",
      "p, q\n",
      ")\n",
      "is referred to as\n",
      "positive\n",
      "if the entry\n",
      "M\n",
      "(\n",
      "p, q\n",
      ")\n",
      "is non-zero. Furthermore, a positive\n",
      "tuple\n",
      "(\n",
      "p, q\n",
      ")\n",
      "is referred to as\n",
      "true positive\n",
      "if the\n",
      "q\n",
      "th\n",
      "query\n",
      "semantically corresponds to the\n",
      "p\n",
      "th\n",
      "audio document, other-\n",
      "wise\n",
      "(\n",
      "p, q\n",
      ")\n",
      "is called\n",
      "false positive\n",
      ". Now, the idea is that pos-\n",
      "itive tuples included in long horizontal line segments within\n",
      "M\n",
      "are likely to be true, whereas isolated positive tuples are\n",
      "likely to be false. Intuitively, our procedure classifies the\n",
      "positive tuples by looking for groups of tuples included in\n",
      "long horizontal line segments (these tuples are classified as\n",
      "true) and discards isolated positives tuples (these tuples are\n",
      "classified as false). Due to space limitations, we do not give\n",
      "technical details and refer to Fig. 2 for an illustration.\n",
      "\n",
      "\n",
      "We have applied this postprocessing procedure to the\n",
      "Beethoven scenario using\n",
      "Q\n",
      "= 3693\n",
      "queries each consisting\n",
      "of\n",
      "7\n",
      "subsequent lines and considering the top match only.\n",
      "As a result,\n",
      "78\n",
      ".\n",
      "42%\n",
      "of the queries were mapped correctly\n",
      "and\n",
      "17\n",
      ".\n",
      "17%\n",
      "of the queries were not mapped (by discarding\n",
      "false positives). The remaining\n",
      "4\n",
      ".\n",
      "41%\n",
      "are incorrect map-\n",
      "pings. Note that the result of this type of postprocessing is\n",
      "the detection rather than the correction of incorrect identifi-\n",
      "cations. Having identified incorrect mappings allows to both\n",
      "further improve the identification process and to automati-\n",
      "cally reveal passages within the sheet music where severe\n",
      "OMR errors have occurred.\n",
      "\n",
      "\n",
      "Rather than identifying incorrect mappings, one may also\n",
      "increase the number of correct identifications. For this, cer-\n",
      "tain tuples are specified as true positives by \u201cfilling\u201d small\n",
      "gaps within horizontal line segments. Thus, OMR queries\n",
      "are assigned to a specific audio document if neighboring\n",
      "OMR queries are consistently assigned to the same audio\n",
      "document. Using\n",
      "k\n",
      "=3\n",
      "in our example increases the num-\n",
      "ber of correct identifications to\n",
      "86\n",
      ".\n",
      "70%\n",
      "(instead of\n",
      "77\n",
      ".\n",
      "74%\n",
      "without postprocessing). Note that there is a natural trade-\n",
      "off between eliminating the incorrect identifications and\n",
      "boosting the correct identifications."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4. Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this paper, we have introduced the problem of mapping\n",
      "sheet music to audio recordings. Based on an automated\n",
      "mapping procedure, we have presented a novel approach for\n",
      "automatically identifying scanned pages of sheet music by\n",
      "means of a given audio collection. Such a procedure, which\n",
      "constitutes an important component in the digitization and\n",
      "annotation process of multimodal music material, is needed\n",
      "for building up the Probado music repository [4] currently\n",
      "set up at Bavarian State Library in Munich, Germany. This\n",
      "music repository, which contains digitized sheet music and\n",
      "audio data for a large collection of classical and romantic\n",
      "piano sonatas (Haydn, Mozart, Beethoven, Schubert, Schu-\n",
      "mann, Chopin, Liszt, Brahms) as well as German 19th cen-\n",
      "turies piano songs, is continuously expanded requiring au-\n",
      "tomated procedures for music processing and annotation.\n",
      "\n",
      "\n",
      "As our experiments show, the proposed procedure for\n",
      "mapping scanned sheet music and audio material works well\n",
      "in the case that there are no severe OMR extraction errors.\n",
      "Our postprocessing procedure allows for automatically re-\n",
      "vealing most of the critical passages containing these OMR\n",
      "errors. In the future, we will use various heuristics to correct\n",
      "typical OMR errors prior to the mapping step. For exam-\n",
      "ple, in the case of piano music, different key signatures for\n",
      "the left and right hand staves can be assumed to be invalid\n",
      "and easily corrected by considering neighboring stave lines.\n",
      "Furthermore, similar to the strategy suggested in [2], one\n",
      "can simultaneously employ various OMR extraction results\n",
      "obtained from different OMR software packages to stabilize\n",
      "the mapping result. Based on these strategies, we expect to\n",
      "achieve a significant improvement of the identification rates\n",
      "reaching the ones reported in our MIDI baseline experiment"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "5. References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Bartsch, M.A., Wakefield, G.H.: Audio thumbnailing of popu-\n",
      "lar music using chroma-based representations. IEEE Trans. on\n",
      "Multimedia\n",
      "7\n",
      "(1), pp. 96\u2013104 (2005).\n",
      "\n",
      "\n",
      "[2] Byrd, D., Schindele, M.: Prospects for improving OMR with\n",
      "multiple recognizers. Proc. ISMIR, Victoria, CA (2006).\n",
      "\n",
      "\n",
      "[3] Choudhury, G., DiLauro, T., Droettboom, M., Fujinaga, I.,\n",
      "Harrington, B., MacMillan, K.: Optical music recognition sys-\n",
      "tem within a large-scale digitization project. Proc. ISMIR, Ply-\n",
      "mouth, MA, US. (2000).\n",
      "\n",
      "\n",
      "[4] Diet, J., Kurth, F.: The Probado Music Repository at the\n",
      "Bavarian State Library. Proc. ISMIR, Vienna, AT (2007).\n",
      "\n",
      "\n",
      "[5] Dunn, J.W., Byrd, D., Notess, M., Riley, J., Scherle, R.: Vari-\n",
      "ations2: Retrieving and using music in an academic setting.\n",
      "Special Issue, Commun. ACM\n",
      "49\n",
      "(8), pp. 53\u201348 (2006).\n",
      "\n",
      "\n",
      "[6] Gracenote:\n",
      "http://www.gracenote.com\n",
      "(2008)\n",
      "\n",
      "\n",
      "[7] Hu, N., Dannenberg, R., Tzanetakis, G.: Polyphonic audio\n",
      "matching and alignment for music retrieval. Proc. IEEE WAS-\n",
      "PAA, New Paltz, US (2003).\n",
      "\n",
      "\n",
      "[8] Jones, G.: SharpEye Music Reader,\n",
      "http://www.visiv.\n",
      "co.uk\n",
      "(2008)\n",
      "\n",
      "\n",
      "[9] Krajewski, E.: DE-PARCON software technology,\n",
      "http://\n",
      "www.deparcon.de\n",
      "(2008)\n",
      "\n",
      "\n",
      "[10] Kurth, F., M\n",
      "\u0308\n",
      "uller, M.: Efficient Index-based Audio Match-\n",
      "ing. IEEE Trans. on Audio, Speech, and Language Processing\n",
      "16\n",
      "(2), pp. 382\u2013395 (2008).\n",
      "\n",
      "\n",
      "[11] Kurth, F., M\n",
      "\u0308\n",
      "uller, M., Fremerey, C., Chang, Y., Clausen, M.:\n",
      "Automated Synchronization of Scanned Sheet Music with Au-\n",
      "dio Recordings. Proc. ISMIR, Vienna, AT (2007).\n",
      "\n",
      "\n",
      "[12] M\n",
      "\u0308\n",
      "uller, M.: Information Retrieval for Music and Motion.\n",
      "Springer (2007).\n",
      "\n",
      "\n",
      "[13] Recordare LLC: Music XML,\n",
      "http://www.recordare.\n",
      "com/xml.html\n",
      "(2008)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}